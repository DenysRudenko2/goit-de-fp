# ğŸ† Olympic Data Engineering Pipeline

## ğŸ“‹ Project Overview

Complete end-to-end data engineering solution for Olympic athlete data processing, featuring both **batch processing** and **real-time streaming** pipelines with multi-layer data lake architecture.

### ğŸ¯ Key Features
- **Dual Pipeline Architecture**: Batch (Data Lake) + Streaming (Real-time)
- **Medallion Architecture**: Landing â†’ Bronze â†’ Silver â†’ Gold layers
- **Modern Stack**: Spark, Kafka, Airflow, MySQL, Docker
- **Production Ready**: Error handling, monitoring, testing, documentation

---

## ğŸ—ï¸ Architecture

### System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BATCH PIPELINE                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Landing  â”‚â†’ â”‚  Bronze  â”‚â†’ â”‚  Silver  â”‚â†’ â”‚     Gold     â”‚      â”‚
â”‚  â”‚  (CSV)   â”‚  â”‚(Parquet) â”‚  â”‚  (Clean) â”‚  â”‚ (Analytics)  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      STREAMING PIPELINE                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  MySQL   â”‚â†’ â”‚  Kafka   â”‚â†’ â”‚  Spark   â”‚â†’ â”‚ Kafka/MySQL  â”‚      â”‚
â”‚  â”‚ (Source) â”‚  â”‚ (Stream) â”‚  â”‚(Process) â”‚  â”‚   (Sink)     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Container Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow    â”‚  â”‚    Spark     â”‚  â”‚    Kafka     â”‚
â”‚  Webserver   â”‚  â”‚   Master     â”‚  â”‚   Broker     â”‚
â”‚  Scheduler   â”‚  â”‚   Worker     â”‚  â”‚  Zookeeper   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                 â†“                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Shared Data Lake Volume                  â”‚
â”‚      /tmp/data_lake (Landingâ†’Gold)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Make (optional, for convenience commands)

### Installation & Setup

```bash
# 1. Clone repository
git clone https://github.com/DenysRudenko2/goit-de-fp.git
cd goit-de-fp

# 2. Configure environment variables
cp .env.example .env
# Edit .env file with your credentials:
# - KAFKA_USERNAME, KAFKA_PASSWORD, KAFKA_BOOTSTRAP_SERVERS
# - MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE
# - USER_NAME (your username for topic naming)

# 3. Start services using Make
make start-streaming    # Start all services
make trigger-dag        # Run batch pipeline
make status            # Check services
make stop              # Stop all services
```

### Environment Variables
All sensitive configuration is managed through environment variables in the `.env` file:

| Variable | Required | Description | Example |
|----------|----------|-------------|---------|
| `USER_NAME` | âœ… | Your username for topic naming | `rudenko` |
| `KAFKA_BOOTSTRAP_SERVERS` | âœ… | Kafka broker addresses | `77.81.230.104:9092` |
| `KAFKA_USERNAME` | âœ… | Kafka SASL username | `admin` |
| `KAFKA_PASSWORD` | âœ… | Kafka SASL password | `your_password` |
| `MYSQL_HOST` | âœ… | MySQL server hostname | `217.61.57.46` |
| `MYSQL_USER` | âœ… | MySQL username | `neo_data_admin` |
| `MYSQL_PASSWORD` | âœ… | MySQL password | `your_mysql_password` |
| `MYSQL_DATABASE` | âœ… | MySQL database name | `olympic_dataset` |
| `FTP_URL` | âŒ | Data source FTP URL | `https://ftp.goit.study/neoversity/` |
| `DATA_LAKE_PATH` | âŒ | Data lake storage path | `/tmp/data_lake` |
| `SPARK_MASTER` | âŒ | Spark master URL | `local[*]` |

**âš ï¸ Important**: The application will fail with clear error messages if required environment variables are not set.

### Access Points
- **Airflow UI**: http://localhost:8090 (airflow/airflow)
- **Spark UI**: http://localhost:8080

---

## ğŸ“Š Batch Pipeline (Data Lake)

### Overview
Multi-hop data lake processing Olympic athlete historical data through medallion architecture.

### Pipeline Stages

#### 1ï¸âƒ£ **Landing to Bronze**
- Downloads CSV files from FTP server
- Converts to Parquet with compression
- Adds metadata (timestamp, source)
- **Input**: athlete_bio.csv (55MB), athlete_event_results.csv (32MB)
- **Output**: 155,861 bio + 316,834 event records

#### 2ï¸âƒ£ **Bronze to Silver**
- Data cleaning and normalization
- Special character removal
- Deduplication (~1,200 duplicates removed)
- Null value handling
- **Output**: Clean, deduplicated data

#### 3ï¸âƒ£ **Silver to Gold**
- Feature engineering (BMI, age calculation)
- Joins bio and event data
- Creates analytics aggregations
- **Outputs**:
  - `athlete_features`: 326,204 enriched records
  - `sport_statistics`: 192 sport/gender combinations

### Running Batch Pipeline

```bash
# Option 1: Via Airflow DAG
make trigger-dag

# Option 2: Direct execution
python dags/landing_to_bronze.py
python dags/bronze_to_silver.py
python dags/silver_to_gold.py
```

---

## ğŸ”„ Streaming Pipeline (Real-time)

### Overview
Real-time processing of athlete data from MySQL through Kafka to analytics outputs.

### Pipeline Components

#### ğŸ“– **Step 1-2: MySQL Source**
- Reads athlete_bio with height/weight filtering
- Validates numeric data quality
- Filters invalid/null values

#### ğŸ“¤ **Step 3: MySQL â†’ Kafka**
- Streams athlete_event_results to Kafka
- JSON serialization with schema
- Topic: `rudenko_athlete_event_results`

#### ğŸ”„ **Step 4-5: Stream Processing**
- Kafka â†’ Spark Structured Streaming
- Joins with athlete_bio data
- Calculates aggregations by sport/medal/country
- Windowed computations with watermarks

#### ğŸ“Š **Step 6: Dual Output (forEachBatch)**
- **6a**: Enriched data â†’ Kafka topic
- **6b**: Aggregations â†’ MySQL table
- Output topic: `rudenko_enriched_athlete_avg`

### Running Streaming Pipeline

```bash
# Start streaming infrastructure
make start-streaming

# Trigger streaming DAG
docker compose exec airflow-webserver \
  airflow dags trigger rudenko_streaming_pipeline

# Or run components manually
python 01_create_kafka_topics.py
python 02_mysql_to_kafka.py
python 03_streaming_processor.py
```

---

## ğŸ“ Project Structure

```
final-project/
â”œâ”€â”€ ğŸ“Š Batch Processing
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ landing_to_bronze.py    # CSV â†’ Parquet conversion
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py     # Data cleaning
â”‚   â”‚   â”œâ”€â”€ silver_to_gold.py       # Feature engineering
â”‚   â”‚   â””â”€â”€ project_solution.py     # Batch DAG
â”‚   â”‚
â”œâ”€â”€ ğŸ”„ Streaming Processing
â”‚   â”œâ”€â”€ 01_create_kafka_topics.py   # Topic setup
â”‚   â”œâ”€â”€ 02_mysql_to_kafka.py        # Data ingestion
â”‚   â”œâ”€â”€ 03_streaming_processor.py   # Stream processing
â”‚   â”œâ”€â”€ 04_test_kafka_read.py       # Testing utility
â”‚   â””â”€â”€ 05_kafka_table_display.py   # Data visualization
â”‚   â”‚
â”œâ”€â”€ ğŸ³ Infrastructure
â”‚   â”œâ”€â”€ docker-compose-streaming.yaml # All services configuration
â”‚   â”œâ”€â”€ Dockerfile-streaming        # Custom Airflow + Spark image
â”‚   â””â”€â”€ Makefile                    # All commands for managing the pipeline
â”‚   â”‚
â””â”€â”€ ğŸ“š Documentation
    â””â”€â”€ README.md                   # This file
```

---

## ğŸ”§ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | Apache Airflow | DAG scheduling and monitoring |
| **Processing** | Apache Spark | Batch and stream processing |
| **Streaming** | Apache Kafka | Message broker for real-time data |
| **Storage** | Parquet/MySQL | Columnar files and relational DB |
| **Containerization** | Docker/Compose | Service isolation and deployment |
| **Languages** | Python/PySpark | Data processing logic |

---

## ğŸ“ˆ Data Quality & Validation

### Quality Metrics
- **Null Analysis**: Height/Weight (27.9%), Age (98.8%), Medals (85.9%)
- **Duplicates**: ~1,200 removed in Silver layer
- **Anomalies**: 2,553 BMI outliers, 3,432 age outliers
- **Completeness**: 326,204 valid joined records from 472,695 total

### Validation Framework
```python
# Schema validation
assert "athlete_id" in df.columns
assert df.schema["height"].dataType == FloatType()

# Business rules
assert df.filter(col("bmi") > 100).count() < threshold
assert df.filter(col("age") < 10).count() == 0

# Data quality
null_percentage = df.filter(col("weight").isNull()).count() / df.count()
assert null_percentage < 0.3
```

---

## ğŸ› ï¸ Operations

### Makefile Commands

```bash
# Service Management
make start           # Start batch infrastructure
make start-streaming # Start streaming infrastructure
make stop           # Stop all services
make restart        # Restart all services

# Pipeline Operations
make trigger-dag    # Run batch pipeline
make test-kafka     # Test Kafka connectivity
make show-data      # Display sample data

# Maintenance
make fix-permissions # Fix volume permissions
make clean          # Remove all data/volumes
make logs           # Show container logs
make status         # Show service status
```

### Monitoring

```bash
# Check pipeline status
docker compose exec airflow-webserver \
  airflow dags state rudenko_data_lake_pipeline

# View streaming metrics
docker compose logs -f spark-master

# Monitor Kafka topics
docker compose exec kafka \
  kafka-console-consumer.sh --topic rudenko_enriched_athlete_avg
```

---

## ğŸ¯ Performance Metrics

### Batch Pipeline
- **Landing â†’ Bronze**: 30-45 seconds
- **Bronze â†’ Silver**: 20-30 seconds
- **Silver â†’ Gold**: 40-50 seconds
- **Total**: ~90-120 seconds

### Streaming Pipeline
- **Latency**: <1 second end-to-end
- **Throughput**: ~10,000 records/second
- **Checkpoint Interval**: 10 seconds

---

## ğŸ› Troubleshooting

### Common Issues & Solutions

| Issue | Solution |
|-------|----------|
| **Permission Denied** | Run `make fix-permissions` |
| **Kafka Connection Failed** | Check network and credentials in config |
| **Spark OOM** | Increase Docker memory allocation |
| **DAG Not Found** | Wait for Airflow to scan DAGs (~30s) |
| **Port Conflicts** | Change ports in docker-compose.yaml |

### Debug Commands
```bash
# Check container logs
docker compose logs [service-name]

# Access container shell
docker compose exec airflow-webserver bash

# View Spark UI
open http://localhost:8080

# Test data lake access
docker compose exec airflow-webserver ls -la /tmp/data_lake/
```

---

## âœ… Project Achievements

### Batch Pipeline
- âœ… Multi-hop medallion architecture
- âœ… Automated data ingestion from FTP
- âœ… Data quality and deduplication
- âœ… Feature engineering (BMI, age)
- âœ… Sport analytics aggregations

### Streaming Pipeline
- âœ… Real-time MySQL â†’ Kafka ingestion
- âœ… Spark Structured Streaming processing
- âœ… Stream-static joins
- âœ… Windowed aggregations
- âœ… Dual output (Kafka + MySQL)

### Infrastructure
- âœ… Full Docker containerization
- âœ… Airflow orchestration
- âœ… Shared volume architecture
- âœ… Production error handling
- âœ… Comprehensive testing suite

---

## ğŸ“š Data Sources

- **FTP Server**: https://ftp.goit.study/neoversity/
- **MySQL Database**: 217.61.57.46:3306/neo_data
- **Kafka Broker**: 77.81.230.104:9092

---

## ğŸ‘¥ Author

**Denys Rudenko**
- GitHub: [@DenysRudenko2](https://github.com/DenysRudenko2)
- Project: [goit-de-fp](https://github.com/DenysRudenko2/goit-de-fp)

---

## ğŸ“„ License

This project is part of the GoIT Data Engineering course final project.

---

**ğŸ‰ All project requirements successfully implemented!**